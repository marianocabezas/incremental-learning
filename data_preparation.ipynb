{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6798c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import os\n",
    "import re\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6e0f6",
   "metadata": {},
   "source": [
    "# CEL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2cb354a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/transcend/ContinualLearning/CEL/sub-VIS00001'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fa13f2235350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdestination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sub-VIS{:05d}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtimepoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimepoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/transcend/ContinualLearning/CEL/sub-VIS00001'"
     ]
    }
   ],
   "source": [
    "visms_path = '/media/transcend/MSReports/CEL/CEL_train/VISMS/'\n",
    "final_path = '/media/transcend/ContinualLearning/CEL/'\n",
    "\n",
    "t1_name = 't1.nii.gz'\n",
    "t1c_name = 't1c.nii.gz'\n",
    "flair_name = 'flair.nii.gz'\n",
    "gt_name = 'cel_new.nii.gz'\n",
    "brain_name = 'brain_mask.nii.gz'\n",
    "\n",
    "visms_sub = [sub for sub in os.listdir(visms_path) if os.path.isdir(os.path.join(visms_path, sub))]\n",
    "\n",
    "for si, sub in enumerate(sorted(visms_sub)):\n",
    "    sub_code = int(re.search(r'\\d+', sub).group())\n",
    "    origin = os.path.join(visms_path, sub)\n",
    "    destination = os.path.join(final_path, 'sub-VIS{:05d}'.format(sub_code))\n",
    "    if not os.path.exists(destination):\n",
    "        os.mkdir(destination)\n",
    "    timepoints = [t for t in os.listdir(origin) if os.path.isdir(os.path.join(origin, t))]\n",
    "    for ti, t in enumerate(timepoints):\n",
    "        print(' '.join([' '] * 300), end='\\r')\n",
    "        print(\n",
    "            'Loading subject {:} ({:}) [{:03d}/{:03d} - {:03d}/{:03d}]'.format(\n",
    "                sub, t, si + 1, len(visms_sub), ti + 1, len(timepoints)\n",
    "            ), end='\\r'\n",
    "        )\n",
    "        t_origin = os.path.join(origin, t)\n",
    "        t_destination = os.path.join(destination, 'ses-{:}'.format(t.lower()))\n",
    "        if not os.path.exists(t_destination):\n",
    "            os.mkdir(t_destination)\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, t1_name),\n",
    "            os.path.join(t_destination, t1_name)\n",
    "        )\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, t1c_name),\n",
    "            os.path.join(t_destination, t1c_name)\n",
    "        )\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, flair_name),\n",
    "            os.path.join(t_destination, flair_name)\n",
    "        )\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, gt_name),\n",
    "            os.path.join(t_destination, 'manual_mask.nii.gz')\n",
    "        )\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, brain_name),\n",
    "            os.path.join(t_destination, brain_name)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88998414",
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_path = '/media/transcend/MSReports/CEL/CEL_train/Batch0/'\n",
    "final_path = '/media/transcend/ContinualLearning/CEL/'\n",
    "\n",
    "b0_sub = [sub for sub in os.listdir(b0_path) if os.path.isdir(os.path.join(b0_path, sub))]\n",
    "\n",
    "t1_name = 't1.nii.gz'\n",
    "t1c_name = 't1c.nii.gz'\n",
    "flair_name = 'flair.nii.gz'\n",
    "gt_name = 'cel_new.nii.gz'\n",
    "brain_name = 'brain_mask.nii.gz'\n",
    "\n",
    "for si, sub in enumerate(sorted(b0_sub)):\n",
    "    sub_code = int(sub)\n",
    "    origin = os.path.join(b0_path, sub)\n",
    "    destination = os.path.join(final_path, 'sub-POS{:05d}'.format(sub_code))\n",
    "    if not os.path.exists(destination):\n",
    "        os.mkdir(destination)\n",
    "    timepoints = [t for t in os.listdir(origin) if os.path.isdir(os.path.join(origin, t))]\n",
    "    for ti, t in enumerate(timepoints):\n",
    "        print(' '.join([' '] * 300), end='\\r')\n",
    "        print(\n",
    "            'Loading subject {:} ({:}) [{:03d}/{:03d} - {:03d}/{:03d}]'.format(\n",
    "                sub, t, si + 1, len(b0_sub), ti + 1, len(timepoints)\n",
    "            ), end='\\r'\n",
    "        )\n",
    "        t_origin = os.path.join(origin, t)\n",
    "        t_destination = os.path.join(destination, 'ses-{:}'.format(t))\n",
    "        if not os.path.exists(t_destination):\n",
    "            os.mkdir(t_destination)\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, t1_name),\n",
    "            os.path.join(t_destination, t1_name)\n",
    "        )\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, t1c_name),\n",
    "            os.path.join(t_destination, t1c_name)\n",
    "        )\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, flair_name),\n",
    "            os.path.join(t_destination, flair_name)\n",
    "        )\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, gt_name),\n",
    "            os.path.join(t_destination, 'manual_mask.nii.gz')\n",
    "        )\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, brain_name),\n",
    "            os.path.join(t_destination, brain_name)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/media/transcend/MSReports/CEL/CEL_testing/'\n",
    "final_path = '/media/transcend/ContinualLearning/CEL/'\n",
    "\n",
    "test_sub = [sub for sub in os.listdir(test_path) if os.path.isdir(os.path.join(test_path, sub))]\n",
    "\n",
    "t1_name = 'T1_pre_reg2_T1_post.nii.gz'\n",
    "t1c_name = 'T1_post.nii.gz'\n",
    "flair_name = 'FLAIR_reg2_T1_post.nii.gz'\n",
    "gt_name = 'cel_new.nii.gz'\n",
    "brain_name = 'T1_post_brain_mask.nii.gz'\n",
    "\n",
    "for si, sub in enumerate(sorted(test_sub)):\n",
    "    sub_code = int(sub)\n",
    "    origin = os.path.join(test_path, sub)\n",
    "    timepoints = [t for t in os.listdir(origin) if os.path.isdir(os.path.join(origin, t))]\n",
    "    volumes = []\n",
    "    for ti, t in enumerate(timepoints):\n",
    "        print(' '.join([' '] * 300), end='\\r')\n",
    "        print(\n",
    "            'Loading subject {:} ({:}) [{:03d}/{:03d} - {:03d}/{:03d}]'.format(\n",
    "                sub, t, si + 1, len(test_sub), ti + 1, len(timepoints)\n",
    "            ), end='\\r'\n",
    "        )\n",
    "        t_origin = os.path.join(origin, t)\n",
    "        mask_nii = nib.load(os.path.join(t_origin, gt_name))\n",
    "        volumes.append(np.sum(mask_nii.get_fdata()) > 0)\n",
    "    positive = np.any(volumes)\n",
    "    if positive:       \n",
    "        destination = os.path.join(final_path, 'sub-POS{:05d}'.format(sub_code))\n",
    "    else:       \n",
    "        destination = os.path.join(final_path, 'sub-NEG{:05d}'.format(sub_code))\n",
    "    if not os.path.exists(destination):\n",
    "        os.mkdir(destination)\n",
    "    for ti, (t, pos_t) in enumerate(zip(timepoints, volumes)):\n",
    "        if (positive and pos_t) or not positive:\n",
    "            t_origin = os.path.join(origin, t)\n",
    "            t_destination = os.path.join(destination, 'ses-{:}'.format(t))\n",
    "            if not os.path.exists(t_destination):\n",
    "                os.mkdir(t_destination)\n",
    "            copyfile(\n",
    "                os.path.join(t_origin, '{:}_{:}_{:}'.format(sub, t, t1_name)),\n",
    "                os.path.join(t_destination, 't1.nii.gz')\n",
    "            )\n",
    "            copyfile(\n",
    "                os.path.join(t_origin, '{:}_{:}_{:}'.format(sub, t, t1c_name)),\n",
    "                os.path.join(t_destination, 't1c.nii.gz')\n",
    "            )\n",
    "            copyfile(\n",
    "                os.path.join(t_origin, '{:}_{:}_{:}'.format(sub, t, flair_name)),\n",
    "                os.path.join(t_destination, 'flair.nii.gz')\n",
    "            )\n",
    "            copyfile(\n",
    "                os.path.join(t_origin, gt_name),\n",
    "                os.path.join(t_destination, 'manual_mask.nii.gz')\n",
    "            )\n",
    "            copyfile(\n",
    "                os.path.join(t_origin, '{:}_{:}_{:}'.format(sub, t, brain_name)),\n",
    "                os.path.join(t_destination, 'brain_mask.nii.gz')\n",
    "            )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfa459",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_path = '/media/transcend/MSReports/CEL/NoCEL/'\n",
    "final_path = '/media/transcend/ContinualLearning/CEL/'\n",
    "\n",
    "neg_sub = [sub for sub in os.listdir(neg_path) if os.path.isdir(os.path.join(neg_path, sub))]\n",
    "\n",
    "t1_name = 'T1_pre_reg2_T1_post.nii.gz'\n",
    "t1c_name = 'T1_post.nii.gz'\n",
    "flair_name = 'FLAIR_reg2_T1_post.nii.gz'\n",
    "gt_name = 'cel_new.nii.gz'\n",
    "brain_name = 'T1_post_brain_mask.nii.gz'\n",
    "\n",
    "for si, sub in enumerate(sorted(neg_sub)):\n",
    "    sub_code = int(sub)\n",
    "    origin = os.path.join(neg_path, sub)\n",
    "    destination = os.path.join(final_path, 'sub-NEG{:05d}'.format(sub_code))\n",
    "    if not os.path.exists(destination):\n",
    "        os.mkdir(destination)\n",
    "    timepoints = [t for t in os.listdir(origin) if os.path.isdir(os.path.join(origin, t))]\n",
    "    for ti, t in enumerate(timepoints):\n",
    "        print(' '.join([' '] * 300), end='\\r')\n",
    "        print(\n",
    "            'Loading subject {:} ({:}) [{:03d}/{:03d} - {:03d}/{:03d}]'.format(\n",
    "                sub, t, si + 1, len(neg_sub), ti + 1, len(timepoints)\n",
    "            ), end='\\r'\n",
    "        )\n",
    "        t_origin = os.path.join(origin, t)\n",
    "        t_destination = os.path.join(destination, 'ses-{:}'.format(t))\n",
    "        if not os.path.exists(t_destination):\n",
    "            os.mkdir(t_destination)\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, '{:}_{:}_{:}'.format(sub, t, t1_name)),\n",
    "            os.path.join(t_destination, 't1.nii.gz')\n",
    "        )\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, '{:}_{:}_{:}'.format(sub, t, t1c_name)),\n",
    "            os.path.join(t_destination, 't1c.nii.gz')\n",
    "        )\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, '{:}_{:}_{:}'.format(sub, t, flair_name)),\n",
    "            os.path.join(t_destination, 'flair.nii.gz')\n",
    "        )\n",
    "        mask_nii = nib.load(os.path.join(t_origin, '{:}_{:}_{:}'.format(sub, t, brain_name)))\n",
    "        no_mask = np.zeros_like(mask_nii.get_fdata())\n",
    "        mask_nii = nib.Nifti1Image(no_mask, mask_nii.get_qform(), mask_nii.header)\n",
    "        mask_nii.to_filename(os.path.join(t_destination, 'manual_mask.nii.gz'))\n",
    "        copyfile(\n",
    "            os.path.join(t_origin, '{:}_{:}_{:}'.format(sub, t, brain_name)),\n",
    "            os.path.join(t_destination, 'brain_mask.nii.gz')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a16194",
   "metadata": {},
   "source": [
    "# Activity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d842dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "msseg_path = '/media/transcend/MSReports/Longitudinal/MICCAI_Challenge2021/training/'\n",
    "final_path = '/media/transcend/ContinualLearning/Activity/'\n",
    "\n",
    "msseg_sub = [\n",
    "    sub for sub in os.listdir(msseg_path)\n",
    "    if os.path.isdir(os.path.join(msseg_path, sub)) and 'CLARA' not in sub\n",
    "]\n",
    "\n",
    "flair1_name = 'flair_time01_on_middle_space_n4.nii.gz'\n",
    "flair2_name = 'flair_time02_on_middle_space_n4.nii.gz'\n",
    "gt_name = 'ground_truth.nii.gz'\n",
    "brain_name = 'brain_mask.nii.gz'\n",
    "\n",
    "for si, sub in enumerate(sorted(msseg_sub)):\n",
    "    sub_code = int(sub)\n",
    "    origin = os.path.join(msseg_path, sub)\n",
    "    mask_nii = nib.load(os.path.join(origin, gt_name))\n",
    "    positive = np.sum(mask_nii.get_fdata()) > 0   \n",
    "    if positive:       \n",
    "        destination = os.path.join(final_path, 'sub-CHP{:05d}'.format(sub_code))\n",
    "    else:       \n",
    "        destination = os.path.join(final_path, 'sub-CHN{:05d}'.format(sub_code))\n",
    "    if not os.path.exists(destination):\n",
    "        os.mkdir(destination)\n",
    "    print(' '.join([' '] * 300), end='\\r')\n",
    "    print(\n",
    "        'Loading subject {:} [{:03d}/{:03d}]'.format(\n",
    "            sub, si + 1, len(msseg_sub),\n",
    "        ), end='\\r'\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(origin, flair1_name),\n",
    "        os.path.join(destination, 'flair01.nii.gz')\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(origin, flair2_name),\n",
    "        os.path.join(destination, 'flair02.nii.gz')\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(origin, gt_name),\n",
    "        os.path.join(destination, 'manual_mask.nii.gz')\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(origin, brain_name),\n",
    "        os.path.join(destination, brain_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f195ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/media/transcend/MSReports/Longitudinal/MICCAI_Challenge2021/private/'\n",
    "final_path = '/media/transcend/ContinualLearning/Activity/'\n",
    "\n",
    "train_sub = [\n",
    "    sub for sub in os.listdir(train_path)\n",
    "    if os.path.isdir(os.path.join(train_path, sub))\n",
    "]\n",
    "\n",
    "flair1_name = 'flair_time01_on_middle_space_n4.nii.gz'\n",
    "flair2_name = 'flair_time02_on_middle_space_n4.nii.gz'\n",
    "gt_name = 'ground_truth.nii.gz'\n",
    "brain_name = 'brain_mask.nii.gz'\n",
    "\n",
    "conversion_dict = {\n",
    "    'GE': 'G3T',\n",
    "    'P1': 'P1T',\n",
    "    'P3': 'P3T',\n",
    "    'S3': 'S3T'\n",
    "}\n",
    "\n",
    "for si, sub in enumerate(sorted(train_sub)):\n",
    "    sub_acq, sub_code = sub.split('_')\n",
    "    origin = os.path.join(train_path, sub)\n",
    "    destination = os.path.join(\n",
    "        final_path, 'sub-{:}{:05d}'.format(conversion_dict[sub_acq], int(sub_code))\n",
    "    )\n",
    "    if not os.path.exists(destination):\n",
    "        os.mkdir(destination)\n",
    "    print(' '.join([' '] * 300), end='\\r')\n",
    "    print(\n",
    "        'Loading subject {:} [{:03d}/{:03d}]'.format(\n",
    "            sub, si + 1, len(train_sub),\n",
    "        ), end='\\r'\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(origin, flair1_name),\n",
    "        os.path.join(destination, 'flair01.nii.gz')\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(origin, flair2_name),\n",
    "        os.path.join(destination, 'flair02.nii.gz')\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(origin, gt_name),\n",
    "        os.path.join(destination, 'manual_mask.nii.gz')\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(origin, brain_name),\n",
    "        os.path.join(destination, brain_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/media/transcend/MSReports/Longitudinal/Longitudinal_testing'\n",
    "final_path = '/media/transcend/ContinualLearning/Activity/'\n",
    "\n",
    "test_sub = [\n",
    "    sub for sub in os.listdir(test_path)\n",
    "    if os.path.isdir(os.path.join(test_path, sub))\n",
    "]\n",
    "\n",
    "gt_name = 'positive_activity_new.nii.gz'\n",
    "brain_name = 'brain_mask.nii.gz'\n",
    "\n",
    "for si, sub in enumerate(sorted(test_sub)):\n",
    "    sub_code = int(sub)\n",
    "    origin = os.path.join(test_path, sub)\n",
    "    timepoints = sorted([t for t in os.listdir(origin) if os.path.isdir(os.path.join(origin, t))])\n",
    "    t = timepoints[-1]\n",
    "    bl = timepoints[0]\n",
    "    t_origin = os.path.join(origin, t)\n",
    "    \n",
    "    mask_nii = nib.load(os.path.join(t_origin, gt_name))\n",
    "    positive = np.sum(mask_nii.get_fdata()) > 0\n",
    "    \n",
    "    if positive:       \n",
    "        destination = os.path.join(final_path, 'sub-POS{:05d}'.format(sub_code))\n",
    "    else:       \n",
    "        destination = os.path.join(final_path, 'sub-NEG{:05d}'.format(sub_code))\n",
    "    if not os.path.exists(destination):\n",
    "        os.mkdir(destination)\n",
    "    \n",
    "    print(' '.join([' '] * 300), end='\\r')\n",
    "    print(\n",
    "        'Loading subject {:} [{:03d}/{:03d}]'.format(\n",
    "            sub, si + 1, len(test_sub),\n",
    "        ), end='\\r'\n",
    "    )\n",
    "        \n",
    "    copyfile(\n",
    "        os.path.join(t_origin, '{:}_{:}_FLAIR_reg2_{:}.nii.gz'.format(sub, bl, t)),\n",
    "        os.path.join(destination, 'flair01.nii.gz')\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(t_origin, '{:}_{:}_FLAIR_n4.nii.gz'.format(sub, t)),\n",
    "        os.path.join(destination, 'flair02.nii.gz')\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(t_origin, gt_name),\n",
    "        os.path.join(destination, 'manual_mask.nii.gz')\n",
    "    )\n",
    "    copyfile(\n",
    "        os.path.join(t_origin, brain_name),\n",
    "        os.path.join(destination, brain_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datasets\n",
    "import models\n",
    "import yaml\n",
    "with open('/media/transcend/IncrementalLearning/cel_unet.yml', 'r') as stream:\n",
    "    try:\n",
    "        config = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import time_to_string, get_mask, get_normalised_image, find_file\n",
    "\n",
    "def get_dataset(experiment_config, subject_list):\n",
    "    d_path = experiment_config['path']\n",
    "    load_start = time.time()\n",
    "    \n",
    "    subjects = []\n",
    "    labels = []\n",
    "    rois = []\n",
    "    for pi, p in enumerate(subject_list):\n",
    "        p_path = os.path.join(d_path, p)\n",
    "        loads = len(subject_list) - pi\n",
    "        load_elapsed = time.time() - load_start\n",
    "        load_eta = loads * load_elapsed / (pi + 1)\n",
    "        if config['multisession']:\n",
    "            sessions = [\n",
    "                session for session in os.listdir(p_path)\n",
    "                if os.path.isdir(os.path.join(p_path, session))\n",
    "            ]\n",
    "            for si, session in enumerate(sessions):                \n",
    "                print(' '.join([' '] * 300), end='\\r')\n",
    "                print(\n",
    "                    'Loading subject {:} [{:}] ({:d}/{:d} - {:d}/{:d}) '\n",
    "                    '{:} ETA {:}'.format(\n",
    "                        p, session, pi + 1, len(subjects),\n",
    "                        si + 1, len(sessions),\n",
    "                        time_to_string(load_elapsed),\n",
    "                        time_to_string(load_eta),\n",
    "                    ), end='\\r'\n",
    "                )\n",
    "                s_path = os.path.join(p_path, session)\n",
    "                roi.append(get_mask(find_file(config['roi'], s_path)))\n",
    "                labels.append(get_mask(find_file(config['labels'], s_path)))\n",
    "                if isinstance(config['files'], tuple):\n",
    "                    images = tuple(load_image_list(s_path, file_i, roi) for file_i in config['files'])\n",
    "                else:\n",
    "                    images = load_image_list(s_path, config['files'], roi)\n",
    "                subjects.append(images)\n",
    "        else:\n",
    "            print(' '.join([' '] * 300), end='\\r')\n",
    "            print(\n",
    "                'Loading subject {:} ({:d}/{:d}) '\n",
    "                '{:} ETA {:}'.format(\n",
    "                    p, pi + 1, len(subjects),\n",
    "                    time_to_string(load_elapsed),\n",
    "                    time_to_string(load_eta),\n",
    "                ), end='\\r'\n",
    "            )\n",
    "            roi.append(get_mask(find_file(config['roi'], p_path)))\n",
    "            labels.append(get_mask(find_file(config['labels'], p_path)))\n",
    "            if isinstance(config['files'], tuple):\n",
    "                images = tuple(load_image_list(p_path, file_i, roi) for file_i in config['files'])\n",
    "            else:\n",
    "                images = load_image_list(p_path, config['files'], roi)\n",
    "            subjects.append(images)\n",
    "\n",
    "    return subjects, labels, rois\n",
    "\n",
    "def load_image_list(path, image_list, roi):\n",
    "    images = [\n",
    "        get_normalised_image(os.path.join(path, image), roi)\n",
    "        for image in image_list\n",
    "    ]\n",
    "    \n",
    "    return np.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjects(experiment_config):\n",
    "    d_path = experiment_config['path']\n",
    "    multitask = False\n",
    "    task = 'Continuum'\n",
    "    tasks = [task]\n",
    "    try:\n",
    "        if config['tasks'] is not None:\n",
    "            multitask = True\n",
    "            tasks = config['tasks']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    subjects = [\n",
    "        patient for patient in os.listdir(d_path)\n",
    "        if os.path.isdir(os.path.join(d_path, patient))\n",
    "    ]\n",
    "    subject_dicts = {\n",
    "        task: [] for task in tasks\n",
    "    }\n",
    "    load_start = time.time()\n",
    "    for pi, p in enumerate(subjects):\n",
    "        if multitask:\n",
    "            for task in tasks:\n",
    "                if task in p:\n",
    "                    break\n",
    "        p_path = os.path.join(d_path, p)\n",
    "        subject_dicts[task].append(p)\n",
    "    return subject_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = get_subjects(config)\n",
    "\n",
    "shuffled_subjects = np.random.permutation(subjects['Continuum'])\n",
    "training_validation = [\n",
    "    len(array.tolist())\n",
    "    for array in np.array_split(\n",
    "        shuffled_subjects,\n",
    "        len(shuffled_subjects) // 20\n",
    "    )\n",
    "]\n",
    "print(training_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(config['files'], tuple):\n",
    "    n_images = len(config['files'][0])\n",
    "else:\n",
    "    n_images = len(config['files']) \n",
    "net = config['network'](\n",
    "    conv_filters=config['filters'],\n",
    "    n_images=n_images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a237f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "json_path = '/media/transcend/IncrementalLearning/Masks/Activity/'\n",
    "naive_file = 'activity_unet-naive_test.f4.s62021.t09.jsom'\n",
    "bl_file = 'activity_unet-baseline_testing.f4.s62021.jsom'\n",
    "\n",
    "json_file = os.path.join(json_path, naive_file)\n",
    "with open(json_file, 'r') as testing_json:\n",
    "    naive_results = json.load(testing_json)\n",
    "    \n",
    "json_file = os.path.join(json_path, bl_file)\n",
    "with open(json_file, 'r') as testing_json:\n",
    "    bl_results = json.load(testing_json)\n",
    "    \n",
    "\n",
    "positive_naive = {\n",
    "    sub: data\n",
    "    for sub, data in naive_results.items() if 'CHN' not in sub and 'NEG' not in sub\n",
    "}\n",
    "chp_naive = {\n",
    "    sub: data\n",
    "    for sub, data in naive_results.items() if 'CHP' in sub\n",
    "}\n",
    "pos_naive = {\n",
    "    sub: data\n",
    "    for sub, data in naive_results.items() if 'POS' in sub\n",
    "}\n",
    "train_naive = {\n",
    "    sub: data\n",
    "    for sub, data in naive_results.items()\n",
    "    if not np.any([tag in sub for tag in ['NEG', 'CHN', 'CHP', 'POS']]).any()\n",
    "}\n",
    "\n",
    "positive_bl = {\n",
    "    sub: data\n",
    "    for sub, data in bl_results.items() if 'CHN' not in sub and 'NEG' not in sub\n",
    "}\n",
    "chp_bl = {\n",
    "    sub: data\n",
    "    for sub, data in bl_results.items() if 'CHP' in sub\n",
    "}\n",
    "pos_bl = {\n",
    "    sub: data\n",
    "    for sub, data in bl_results.items() if 'POS' in sub\n",
    "}\n",
    "train_bl = {\n",
    "    sub: data\n",
    "    for sub, data in bl_results.items()\n",
    "    if not np.any([tag in sub for tag in ['NEG', 'CHN', 'CHP', 'POS']]).any()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be910de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "def plot_bands(\n",
    "    x, y, yinf, ysup, ax, xmin=None, xmax=None, ymin=0, ymax=1,\n",
    "    title='', xlabel='Epoch', ylabel='Metric', legend=None\n",
    "):\n",
    "    # Init\n",
    "    if xmin is None:\n",
    "        xmin = np.min(x)\n",
    "    if xmax is None:\n",
    "        xmax = np.max(x)\n",
    "    if ymin is None:\n",
    "        ymin = np.min(yinf)\n",
    "    if ymax is None:\n",
    "        ymax = np.max(ysup)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    colomap = ['b', 'g', 'c', 'r', 'm', 'y', 'k']\n",
    "\n",
    "    if yinf is not None and ysup is not None:\n",
    "        for yi, yinfi, ysupi, ci in zip(y, yinf, ysup, colomap):\n",
    "            ax.plot(x, yi, '-', color=ci)\n",
    "            ax.fill_between(x, yinfi, ysupi, alpha=0.2, color=ci)\n",
    "    else:\n",
    "        for yi, ci in zip(y, colomap):\n",
    "            ax.plot(x, yi, '-', color=ci, linewidth=2.0)\n",
    "\n",
    "    ax.set_xlim(xmin=xmin, xmax=xmax)\n",
    "    ax.set_ylim(ymin=ymin, ymax=ymax)\n",
    "\n",
    "    if legend is not None:\n",
    "        ax.legend(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3feb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(baseline, naive, dataset, filename, n_tasks=10):\n",
    "    seeds = [42, 80702, 74794, 62021, 48497]\n",
    "    dsc_bl = [\n",
    "        [\n",
    "            [\n",
    "                2 * tpv / (2 * tpv + fnv + fpv)\n",
    "                for tpv, fnv, fpv in zip(\n",
    "                    sub_data[seed]['TPV'],\n",
    "                    sub_data[seed]['FNV'],\n",
    "                    sub_data[seed]['FPV']\n",
    "                )\n",
    "            ] * 10\n",
    "            for sub_data in baseline.values()\n",
    "        ]\n",
    "        for seed in seeds\n",
    "    ]\n",
    "    dtpf_bl = [\n",
    "        [\n",
    "            [\n",
    "                tpr / gtr\n",
    "                for tpr, gtr in zip(\n",
    "                    sub_data[seed]['TPR'],\n",
    "                    sub_data[seed]['GTR']\n",
    "                )\n",
    "            ] * 10\n",
    "            for sub_data in baseline.values()\n",
    "        ]\n",
    "        for seed in seeds\n",
    "    ]\n",
    "    f1_bl = [\n",
    "        [\n",
    "            [\n",
    "                2 * tpr / (r + gtr)\n",
    "                for tpr, gtr, r in zip(\n",
    "                    sub_data[seed]['TPR'],\n",
    "                    sub_data[seed]['GTR'],\n",
    "                    sub_data[seed]['R']\n",
    "                )\n",
    "            ] * 10\n",
    "            for sub_data in baseline.values()\n",
    "        ]\n",
    "        for seed in seeds\n",
    "    ]\n",
    "\n",
    "    dsc_naive = [\n",
    "        [\n",
    "            [\n",
    "                2 * tpv / (2 * tpv + fnv + fpv)\n",
    "                for tpv, fnv, fpv in zip(\n",
    "                    sub_data[seed]['TPV'],\n",
    "                    sub_data[seed]['FNV'],\n",
    "                    sub_data[seed]['FPV']\n",
    "                )\n",
    "            ]\n",
    "            for sub_data in naive.values()\n",
    "        ]\n",
    "        for seed in seeds\n",
    "    ]\n",
    "    dtpf_naive = [\n",
    "        [\n",
    "            [\n",
    "                tpr / gtr\n",
    "                for tpr, gtr in zip(\n",
    "                    sub_data[seed]['TPR'],\n",
    "                    sub_data[seed]['GTR']\n",
    "                )\n",
    "            ]\n",
    "            for sub_data in naive.values()\n",
    "        ]\n",
    "        for seed in seeds\n",
    "    ]\n",
    "    f1_naive = [\n",
    "        [\n",
    "            [\n",
    "                2 * tpr / (r + gtr)\n",
    "                for tpr, gtr, r in zip(\n",
    "                    sub_data[seed]['TPR'],\n",
    "                    sub_data[seed]['GTR'],\n",
    "                    sub_data[seed]['R']\n",
    "                )\n",
    "            ]\n",
    "            for sub_data in naive.values()\n",
    "        ]\n",
    "        for seed in seeds\n",
    "    ]\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    mean_tasks_bl = np.mean(dsc_bl, axis=1)\n",
    "    mean_tasks_naive = np.mean(dsc_naive, axis=1)\n",
    "    x = list(range(n_tasks))\n",
    "    y = np.stack([\n",
    "        np.mean(mean_tasks_bl, axis=0),\n",
    "        np.mean(mean_tasks_naive, axis=0)\n",
    "    ], axis=0)\n",
    "    yinf = np.stack([\n",
    "        np.min(mean_tasks_bl, axis=0),\n",
    "        np.min(mean_tasks_naive, axis=0)\n",
    "    ], axis=0)\n",
    "    ysup = np.stack([\n",
    "        np.max(mean_tasks_bl, axis=0),\n",
    "        np.max(mean_tasks_naive, axis=0)\n",
    "    ], axis=0)\n",
    "    plot_bands(\n",
    "        x, y, yinf, ysup, ax, title='DSC evolution ({:})'.format(dataset), xlabel='Task', ylabel='DSC',\n",
    "        legend=['Baseline', 'Naive']\n",
    "    )\n",
    "\n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    mean_tasks_bl = np.mean(dtpf_bl, axis=1)\n",
    "    mean_tasks_naive = np.mean(dtpf_naive, axis=1)\n",
    "    x = list(range(n_tasks))\n",
    "    y = np.stack([\n",
    "        np.mean(mean_tasks_bl, axis=0),\n",
    "        np.mean(mean_tasks_naive, axis=0)\n",
    "    ], axis=0)\n",
    "    yinf = np.stack([\n",
    "        np.min(mean_tasks_bl, axis=0),\n",
    "        np.min(mean_tasks_naive, axis=0)\n",
    "    ], axis=0)\n",
    "    ysup = np.stack([\n",
    "        np.max(mean_tasks_bl, axis=0),\n",
    "        np.max(mean_tasks_naive, axis=0)\n",
    "    ], axis=0)\n",
    "    plot_bands(\n",
    "        x, y, yinf, ysup, ax, title='TPF evolution ({:})'.format(dataset), xlabel='Task', ylabel='TPF%',\n",
    "        legend=['Baseline', 'Naive']\n",
    "    )\n",
    "\n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    mean_tasks_bl = np.mean(f1_bl, axis=1)\n",
    "    mean_tasks_naive = np.mean(f1_naive, axis=1)\n",
    "    x = list(range(n_tasks))\n",
    "    y = np.stack([\n",
    "        np.mean(mean_tasks_bl, axis=0),\n",
    "        np.mean(mean_tasks_naive, axis=0)\n",
    "    ], axis=0)\n",
    "    yinf = np.stack([\n",
    "        np.min(mean_tasks_bl, axis=0),\n",
    "        np.min(mean_tasks_naive, axis=0)\n",
    "    ], axis=0)\n",
    "    ysup = np.stack([\n",
    "        np.max(mean_tasks_bl, axis=0),\n",
    "        np.max(mean_tasks_naive, axis=0)\n",
    "    ], axis=0)\n",
    "    plot_bands(\n",
    "        x, y, yinf, ysup, ax, title='F1 evolution ({:})'.format(dataset), xlabel='Task', ylabel='F1',\n",
    "        legend=['Baseline', 'Naive']\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc327714",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_metrics(positive_bl, positive_naive, 'Continuum', os.path.join(json_path, 'continuum_test_plots.png'))\n",
    "plot_metrics(chp_bl, chp_naive, 'Challenge', os.path.join(json_path, 'chp_test_plots.png'))\n",
    "plot_metrics(pos_bl, pos_naive, 'Positive', os.path.join(json_path, 'pos_test_plots.png'))\n",
    "plot_metrics(train_bl, train_naive, 'Training', os.path.join(json_path, 'trainset_test_plots.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed048e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "def plot_train_metrics(baseline, naive, dataset, filename, seed, fold, n_tasks=10):\n",
    "    dsc_bl = np.swapaxes([\n",
    "        np.mean([\n",
    "            [\n",
    "                2 * tpv / (2 * tpv + fnv + fpv)\n",
    "                for tpv, fnv, fpv in zip(\n",
    "                    sub_data['TPV'][1:],\n",
    "                    sub_data['FNV'][1:],\n",
    "                    sub_data['FPV'][1:]\n",
    "                )\n",
    "            ]\n",
    "            for sub_data in task.values()\n",
    "        ], axis=0)\n",
    "        for task in baseline\n",
    "    ], 0, 1)\n",
    "    dtpf_bl = np.swapaxes([\n",
    "        np.mean([\n",
    "            [\n",
    "                tpr / gtr\n",
    "                for tpr, gtr in zip(\n",
    "                    sub_data['TPR'][1:],\n",
    "                    sub_data['GTR'][1:]\n",
    "                )\n",
    "            ]\n",
    "            for sub_data in task.values()\n",
    "        ], axis=0)\n",
    "        for task in baseline\n",
    "    ], 0, 1)\n",
    "    f1_bl = np.swapaxes([\n",
    "        np.mean([\n",
    "            [\n",
    "                2 * tpr / (r + gtr)\n",
    "                for tpr, gtr, r in zip(\n",
    "                    sub_data['TPR'][1:],\n",
    "                    sub_data['GTR'][1:],\n",
    "                    sub_data['R'][1:]\n",
    "                )\n",
    "            ]\n",
    "            for sub_data in task.values()\n",
    "        ], axis=0)\n",
    "        for task in baseline\n",
    "    ], 0, 1)\n",
    "\n",
    "    dsc_naive = np.swapaxes([\n",
    "        np.mean([\n",
    "            [\n",
    "                2 * tpv / (2 * tpv + fnv + fpv)\n",
    "                for tpv, fnv, fpv in zip(\n",
    "                    sub_data['TPV'][1:],\n",
    "                    sub_data['FNV'][1:],\n",
    "                    sub_data['FPV'][1:]\n",
    "                )\n",
    "            ]\n",
    "            for sub_data in task.values()\n",
    "        ], axis=0)\n",
    "        for task in naive\n",
    "    ], 0, 1)\n",
    "    dtpf_naive = np.swapaxes([\n",
    "        np.mean([\n",
    "            [\n",
    "                tpr / gtr\n",
    "                for tpr, gtr in zip(\n",
    "                    sub_data['TPR'][1:],\n",
    "                    sub_data['GTR'][1:]\n",
    "                )\n",
    "            ]\n",
    "            for sub_data in task.values()\n",
    "        ], axis=0)\n",
    "        for task in naive\n",
    "    ], 0, 1)\n",
    "    f1_naive = np.swapaxes([\n",
    "        np.mean([\n",
    "            [\n",
    "                2 * tpr / (r + gtr)\n",
    "                for tpr, gtr, r in zip(\n",
    "                    sub_data['TPR'][1:],\n",
    "                    sub_data['GTR'][1:],\n",
    "                    sub_data['R'][1:]\n",
    "                )\n",
    "            ]\n",
    "            for sub_data in task.values()\n",
    "        ], axis=0)\n",
    "        for task in naive\n",
    "    ], 0, 1)\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    sn.heatmap(\n",
    "        np.concatenate([dsc_bl, dsc_naive]), cmap='jet', vmin=0,\n",
    "        xticklabels=['Task{:02d}'.format(si) for si in range(len(naive))],\n",
    "        yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(len(naive))]\n",
    "    )\n",
    "    plt.title('DSC (segmentation) - Seed {:} - Fold {:02d}'.format(seed, fold))\n",
    "\n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    sn.heatmap(\n",
    "        np.concatenate([dtpf_bl, dtpf_naive]), cmap='jet', vmin=0,\n",
    "        xticklabels=['Task{:02d}'.format(si) for si in range(len(naive))],\n",
    "        yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(len(naive))]\n",
    "    )\n",
    "    plt.title('TPF% (detection) - Seed {:} - Fold {:02d}'.format(seed, fold))\n",
    "\n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    sn.heatmap(\n",
    "        np.concatenate([f1_bl, f1_naive]), cmap='jet', vmin=0,\n",
    "        xticklabels=['Task{:02d}'.format(si) for si in range(len(naive))],\n",
    "        yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(len(naive))]\n",
    "    )\n",
    "    plt.title('F1 score (detection) - Seed {:} - Fold {:02d}'.format(seed, fold))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "    return dsc_bl, dsc_naive, dtpf_bl, dtpf_naive, f1_bl, f1_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664b9c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dsc_bl_list = []\n",
    "dsc_naive_list = []\n",
    "dtpf_bl_list = []\n",
    "dtpf_naive_list = []\n",
    "f1_bl_list = []\n",
    "f1_naive_list = []\n",
    "\n",
    "for seed in ['42', '80702', '74794']:\n",
    "    for fold in range(5):\n",
    "        naive_file = 'activity_unet-naive-training.f{:d}.s{:}.t09.jsom'.format(fold, seed)\n",
    "        bl_file = 'activity_unet-baseline-training.f{:d}.s{:}.jsom'.format(fold, seed)\n",
    "\n",
    "        json_file = os.path.join(json_path, naive_file)\n",
    "        with open(json_file, 'r') as testing_json:\n",
    "            naive_results = json.load(testing_json)\n",
    "\n",
    "        json_file = os.path.join(json_path, bl_file)\n",
    "        with open(json_file, 'r') as testing_json:\n",
    "            bl_results = json.load(testing_json)\n",
    "\n",
    "        positive_naive = [{\n",
    "            sub: data\n",
    "            for sub, data in task_results.items() if 'CHN' not in sub and 'NEG' not in sub\n",
    "        } for task_results in naive_results]\n",
    "        chp_naive = [{\n",
    "            sub: data\n",
    "            for sub, data in task_results.items() if 'CHP' in sub\n",
    "        } for task_results in naive_results]\n",
    "        pos_naive = [{\n",
    "            sub: data\n",
    "            for sub, data in task_results.items() if 'POS' in sub\n",
    "        } for task_results in naive_results]\n",
    "        train_naive = [{\n",
    "            sub: data\n",
    "            for sub, data in task_results.items()\n",
    "            if not np.any([tag in sub for tag in ['NEG', 'CHN', 'CHP', 'POS']]).any()\n",
    "        } for task_results in naive_results]\n",
    "\n",
    "        positive_bl = [{\n",
    "            sub: data\n",
    "            for sub, data in task_results.items() if 'CHN' not in sub and 'NEG' not in sub\n",
    "        } for task_results in bl_results]\n",
    "        chp_bl = [{\n",
    "            sub: data\n",
    "            for sub, data in task_results.items() if 'CHP' in sub\n",
    "        } for task_results in bl_results]\n",
    "        pos_bl = [{\n",
    "            sub: data\n",
    "            for sub, data in task_results.items() if 'POS' in sub\n",
    "        } for task_results in bl_results]\n",
    "        train_bl = [{\n",
    "            sub: data\n",
    "            for sub, data in task_results.items()\n",
    "            if not np.any([tag in sub for tag in ['NEG', 'CHN', 'CHP', 'POS']]).any()\n",
    "        } for task_results in bl_results]\n",
    "\n",
    "        dsc_bl, dsc_naive, dtpf_bl, dtpf_naive, f1_bl, f1_naive = plot_train_metrics(\n",
    "            positive_bl, positive_naive, 'Continuum',\n",
    "            os.path.join(json_path, 'continuum_train_plots_f{:d}_s{:}.png'.format(fold, seed)),\n",
    "            seed, fold\n",
    "        )\n",
    "        \n",
    "        dsc_bl_list.append(dsc_bl)\n",
    "        dsc_naive_list.append(dsc_naive)\n",
    "        dtpf_bl_list.append(dtpf_bl)\n",
    "        dtpf_naive_list.append(dtpf_naive)\n",
    "        f1_bl_list.append(f1_bl)\n",
    "        f1_naive_list.append(f1_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e1743f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 6))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "sn.heatmap(\n",
    "    np.concatenate([np.min(dsc_bl_list, axis=0), np.min(dsc_naive_list, axis=0)]),\n",
    "    cmap='jet', vmin=0, xticklabels=['Task{:02d}'.format(si) for si in range(10)],\n",
    "    yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(10)]\n",
    ")\n",
    "plt.title('DSC (segmentation) - Minimum')\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "sn.heatmap(\n",
    "    np.concatenate([np.min(dtpf_bl_list, axis=0), np.min(dtpf_naive_list, axis=0)]),\n",
    "    cmap='jet', vmin=0, xticklabels=['Task{:02d}'.format(si) for si in range(10)],\n",
    "    yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(10)]\n",
    ")\n",
    "plt.title('TPF% (detection) - Minimum')\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "sn.heatmap(\n",
    "    np.concatenate([np.min(f1_bl_list, axis=0), np.min(f1_naive_list, axis=0)]),\n",
    "    cmap='jet', vmin=0, xticklabels=['Task{:02d}'.format(si) for si in range(10)],\n",
    "    yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(10)]\n",
    ")\n",
    "plt.title('F1 score (detection) - Minimum')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(json_path, 'continuum_train_plots_min.png'.format(fold, seed)))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "sn.heatmap(\n",
    "    np.concatenate([np.mean(dsc_bl_list, axis=0), np.mean(dsc_naive_list, axis=0)]),\n",
    "    cmap='jet', vmin=0, xticklabels=['Task{:02d}'.format(si) for si in range(10)],\n",
    "    yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(10)]\n",
    ")\n",
    "plt.title('DSC (segmentation) - Average')\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "sn.heatmap(\n",
    "    np.concatenate([np.mean(dtpf_bl_list, axis=0), np.mean(dtpf_naive_list, axis=0)]),\n",
    "    cmap='jet', vmin=0, xticklabels=['Task{:02d}'.format(si) for si in range(10)],\n",
    "    yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(10)]\n",
    ")\n",
    "plt.title('TPF% (detection) - Average')\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "sn.heatmap(\n",
    "    np.concatenate([np.mean(f1_bl_list, axis=0), np.mean(f1_naive_list, axis=0)]),\n",
    "    cmap='jet', vmin=0, xticklabels=['Task{:02d}'.format(si) for si in range(10)],\n",
    "    yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(10)]\n",
    ")\n",
    "plt.title('F1 score (detection) - Average')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(json_path, 'continuum_train_plots_mean.png'.format(fold, seed)))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "sn.heatmap(\n",
    "    np.concatenate([np.max(dsc_bl_list, axis=0), np.max(dsc_naive_list, axis=0)]),\n",
    "    cmap='jet', vmin=0, xticklabels=['Task{:02d}'.format(si) for si in range(10)],\n",
    "    yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(10)]\n",
    ")\n",
    "plt.title('DSC (segmentation) - Maximum')\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "sn.heatmap(\n",
    "    np.concatenate([np.max(dtpf_bl_list, axis=0), np.max(dtpf_naive_list, axis=0)]),\n",
    "    cmap='jet', vmin=0, xticklabels=['Task{:02d}'.format(si) for si in range(10)],\n",
    "    yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(10)]\n",
    ")\n",
    "plt.title('TPF% (detection) - Maximum')\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "sn.heatmap(\n",
    "    np.concatenate([np.max(f1_bl_list, axis=0), np.max(f1_naive_list, axis=0)]),\n",
    "    cmap='jet', vmin=0, xticklabels=['Task{:02d}'.format(si) for si in range(10)],\n",
    "    yticklabels=['Baseline'] + ['Step{:02d}'.format(si) for si in range(10)]\n",
    ")\n",
    "plt.title('F1 score (detection) - Maximum')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(json_path, 'continuum_train_plots_max.png'.format(fold, seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a661e5",
   "metadata": {},
   "source": [
    "## FeTS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0fef4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 266 (17-12) [340/341]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from shutil import copyfile\n",
    "from utils import time_to_string, get_mask, get_normalised_image, find_file\n",
    "partition1 = '/media/transcend/MICCAI_FeTS2021_TrainingData/partitioning_1.csv'\n",
    "partition2 = '/media/transcend/MICCAI_FeTS2021_TrainingData/partitioning_2.csv'\n",
    "\n",
    "fets_path = '/media/transcend/MICCAI_FeTS2021_TrainingData/'\n",
    "final_path = '/media/transcend/IncrementalLearning/FeTS/'\n",
    "\n",
    "brain_name = 't1.nii.gz'\n",
    "\n",
    "df1 = pd.read_csv(partition1)\n",
    "df2 = pd.read_csv(partition2)\n",
    "\n",
    "ids = []\n",
    "\n",
    "count_id1 = pd.read_csv(partition1).groupby('Partition_ID').count()\n",
    "count_id2 = pd.read_csv(partition2).groupby('Partition_ID').count()\n",
    "\n",
    "valid_ids = count_id1[count_id1['Subject_ID'] >= 5].index.tolist()\n",
    "\n",
    "for row, row_data in df1.iterrows():\n",
    "    subject = row_data['Subject_ID']\n",
    "    split1_id = row_data['Partition_ID']\n",
    "    if split1_id in valid_ids:\n",
    "        origin = os.path.join(fets_path, subject)\n",
    "        split2_id = df2[df2['Subject_ID'] == subject].iloc[0]['Partition_ID']\n",
    "        fets_id = 'FeTS{:02d}{:02d}'.format(split1_id, split2_id)\n",
    "        sub_name = subject.split('_')[-1]\n",
    "        out_name = 'sub-{:}_{:}'.format(fets_id, sub_name)\n",
    "\n",
    "        destination = os.path.join(\n",
    "            final_path, 'sub-{:}_{:}'.format(fets_id, subject.split('_')[-1])\n",
    "        )\n",
    "        if not os.path.exists(destination):\n",
    "            os.mkdir(destination)\n",
    "        print(' '.join([' '] * 300), end='\\r')\n",
    "        print(\n",
    "            'Loading subject {:} ({:}-{:}) [{:03d}/{:03d}]'.format(\n",
    "                sub_name, split1_id, split2_id, row + 1, len(df1)\n",
    "            ), end='\\r'\n",
    "        )\n",
    "        for file in os.listdir(origin):\n",
    "            copyfile(\n",
    "                os.path.join(origin, file), \n",
    "                os.path.join(destination, file)\n",
    "            )\n",
    "        brain_nii = nib.load(find_file(brain_name, origin))\n",
    "        mask_nii = nib.Nifti1Image(\n",
    "            (brain_nii.get_fdata() > 0).astype(np.uint8),\n",
    "            brain_nii.get_qform(), brain_nii.header\n",
    "        )\n",
    "        mask_nii.to_filename(os.path.join(destination, 'brain_mask.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683de6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
